// Syst√®me de tracking historique avec Azure Table Storage (30x moins cher que Redis)
// Fallback vers m√©moire volatile si Table Storage non disponible
let responseHistory;
try {
    responseHistory = require('../utils/tableStorage');
} catch (error) {
    // Fallback vers m√©moire volatile si module non disponible
    responseHistory = {
        initialized: false,
        entries: [],
        async initialize() { this.initialized = true; },
        add(entry) {
            this.entries.push({ ...entry, timestamp: new Date().toISOString() });
            if (this.entries.length > 100) this.entries.shift();
        },
        getStats() {
            if (this.entries.length === 0) return { avgConfidence: 0.7, avgValidation: 1.0, sampleSize: 0 };
            const avgConfidence = this.entries.reduce((sum, e) => sum + e.confidence, 0) / this.entries.length;
            const avgValidation = this.entries.reduce((sum, e) => sum + e.validation, 0) / this.entries.length;
            return {
                avgConfidence: Math.round(avgConfidence * 100) / 100,
                avgValidation: Math.round(avgValidation * 100) / 100,
                sampleSize: this.entries.length
            };
        },
        getAdaptiveThreshold() {
            const stats = this.getStats();
            if (stats.avgValidation < 0.8) return 0.25;
            else if (stats.avgConfidence > 0.85) return 0.35;
            return 0.30;
        }
    };
}

// RAG System pour recherche vectorielle et fact-checking interne
let ragSystem;
try {
    const RAGSystem = require('../utils/ragSystem');
    ragSystem = new RAGSystem();
} catch (error) {
    console.log('‚ö†Ô∏è  RAG System non disponible - fonctionnalit√© d√©sactiv√©e');
    ragSystem = {
        enabled: false,
        async search() { return []; },
        async verifyClaim() { return { verified: false, found: false }; },
        async enrichContext() { return { enriched: false }; }
    };
}

// Fact-Checker avec Google Fact Check Tools API
let factChecker;
try {
    const FactChecker = require('../utils/factChecker');
    factChecker = new FactChecker();
} catch (error) {
    console.log('‚ö†Ô∏è  Fact-Checker non disponible - fonctionnalit√© d√©sactiv√©e');
    factChecker = {
        enabled: false,
        async checkText() { return { checked: false }; },
        async checkClaim() { return { checked: false }; }
    };
}

// Protection contre accumulation d'hallucinations
let hallucinationProtection;
try {
    hallucinationProtection = require('../utils/hallucinationProtection');
} catch (error) {
    console.log('‚ö†Ô∏è  Hallucination Protection non disponible - fonctionnalit√© d√©sactiv√©e');
    hallucinationProtection = {
        analyzeConversationRisk() {
            return {
                level: 'safe',
                stats: { avgHI: 0, maxHI: 0, recentAvgHI: 0, totalMessages: 0, highRiskCount: 0, trend: 'stable' },
                action: { type: 'NONE', message: '', description: '', actions: [], icon: '', color: '' },
                shouldIntervene: false,
                shouldBlock: false
            };
        }
    };
}

// üöÄ CACHE SIMPLE : R√©ponses rapides pour questions fr√©quentes
const responseCache = new Map();
const CACHE_TTL = 3600000; // 1 heure en millisecondes
const MAX_CACHE_SIZE = 100;

function getCacheKey(message, historyLength) {
    // Normaliser le message pour le cache (lowercase, trim, pas d'historique pour les questions simples)
    const normalizedMsg = message.toLowerCase().trim();
    // Si pas d'historique, utiliser juste le message
    if (historyLength === 0) {
        return normalizedMsg;
    }
    // Avec historique, inclure la taille pour √©viter les collisions
    return `${normalizedMsg}:${historyLength}`;
}

function getCachedResponse(cacheKey) {
    const cached = responseCache.get(cacheKey);
    if (cached && (Date.now() - cached.timestamp < CACHE_TTL)) {
        return cached.response;
    }
    if (cached) {
        responseCache.delete(cacheKey); // Expirer
    }
    return null;
}

function setCachedResponse(cacheKey, response) {
    // Limiter la taille du cache
    if (responseCache.size >= MAX_CACHE_SIZE) {
        const firstKey = responseCache.keys().next().value;
        responseCache.delete(firstKey);
    }
    responseCache.set(cacheKey, {
        response: response,
        timestamp: Date.now()
    });
}

module.exports = async function (context, req) {
    // Initialiser le storage au premier appel
    if (!responseHistory.initialized) {
        await responseHistory.initialize();
    }
    
    // startTime sera d√©clar√© plus tard apr√®s validation du message
    context.log('Axilum AI - Hallucination Detection started');
    context.log('Request method:', req.method);
    context.log('Request body:', JSON.stringify(req.body));

    try {
        const userMessage = req.body?.message || req.query?.message;
        
        if (!userMessage) {
            context.log.warn('No message provided in request');
            context.res = {
                status: 400,
                headers: { 'Content-Type': 'application/json' },
                body: { error: "Message is required", received: req.body }
            };
            return;
        }

        const apiKey = process.env.AZURE_AI_API_KEY;
        const endpoint = 'https://saidzeghidi-2025-1-resource.cognitiveservices.azure.com';
        const deploymentName = 'gpt-5.1-chat';

        if (!apiKey) {
            context.log.error('AZURE_AI_API_KEY not configured');
            context.res = {
                status: 500,
                headers: { 'Content-Type': 'application/json' },
                body: { 
                    error: "API Key not configured",
                    hint: "Please configure AZURE_AI_API_KEY in Azure Static Web App settings"
                }
            };
            return;
        }

        // üöÄ PERFORMANCE : D√©marrer le timer
        const startTime = Date.now();
        
        context.log('Using GPT-5.1 with hallucination detection');
        context.log('Message length:', userMessage.length);
        
        // üöÄ CACHE CHECK : V√©rifier si la r√©ponse est d√©j√† en cache
        const conversationHistory = req.body.history || [];
        const cacheKey = getCacheKey(userMessage, conversationHistory.length);
        const cachedResponse = getCachedResponse(cacheKey);
        
        if (cachedResponse) {
            const cacheTime = Date.now() - startTime;
            context.log(`‚ö° CACHE HIT! Response time: ${cacheTime}ms`);
            context.res = {
                status: 200,
                headers: { 
                    'Content-Type': 'application/json',
                    'X-Cache': 'HIT',
                    'X-Response-Time': `${cacheTime}ms`
                },
                body: {
                    ...cachedResponse,
                    cached: true,
                    cacheResponseTime: `${cacheTime}ms`
                }
            };
            return;
        }
        
        context.log('üíæ Cache miss, generating new response');

        // üé® D√âTECTION G√âN√âRATION D'IMAGES : V√©rifier si l'utilisateur demande une image
        const imageKeywords = ['photo', 'image', 'picture', 'g√©n√®re', 'g√©nerer', 'cr√©e', 'cr√©er', 'dessine', 'dessiner', 'illustre', 'illustration', 'visualise'];
        const messageWords = userMessage.toLowerCase().split(/\s+/);
        const isImageRequest = imageKeywords.some(keyword => 
            messageWords.some(word => word.includes(keyword))
        );

        // Si demande d'image d√©tect√©e, g√©n√©rer avec Pollinations.ai
        if (isImageRequest && (userMessage.toLowerCase().includes('une') || userMessage.toLowerCase().includes('un'))) {
            context.log('üé® Image generation request detected');
            
            try {
                // Extraire le prompt de g√©n√©ration d'image
                const imagePrompt = userMessage
                    .replace(/g√©n√®re|g√©nerer|cr√©e|cr√©er|dessine|dessiner|illustre|illustration|visualise|photo|image|picture/gi, '')
                    .replace(/une|un|de|d'|du/gi, '')
                    .trim();
                
                context.log('üé® Image prompt:', imagePrompt);
                
                // Appeler l'API de g√©n√©ration d'image
                const generateImageUrl = process.env.NODE_ENV === 'production' 
                    ? 'https://proud-mushroom-019836d03.3.azurestaticapps.net/api/generate-image'
                    : 'http://localhost:7071/api/generate-image';
                
                const imageResponse = await fetch(generateImageUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ 
                        prompt: imagePrompt || userMessage,
                        width: 1024,
                        height: 1024
                    })
                });
                
                if (!imageResponse.ok) {
                    throw new Error(`Image generation failed: ${imageResponse.status}`);
                }
                
                const imageData = await imageResponse.json();
                const processingTime = Date.now() - startTime;
                
                context.log('‚úÖ Image generated:', imageData.imageUrl);
                
                context.res = {
                    status: 200,
                    headers: { 'Content-Type': 'application/json' },
                    body: {
                        text: `Voici l'image g√©n√©r√©e pour : "${imagePrompt || userMessage}" üé®`,
                        imageUrl: imageData.imageUrl,
                        imageGenerated: true,
                        prompt: imagePrompt || userMessage,
                        model: imageData.model,
                        hallucinationIndex: 0,
                        contextHistoryRatio: 0,
                        responseTime: `${processingTime}ms`,
                        cached: false
                    }
                };
                return;
                
            } catch (error) {
                context.log.error('‚ùå Image generation error:', error);
                context.res = {
                    status: 200,
                    headers: { 'Content-Type': 'application/json' },
                    body: {
                        text: `D√©sol√©, je n'ai pas pu g√©n√©rer l'image. Erreur : ${error.message}`,
                        hallucinationIndex: 0,
                        contextHistoryRatio: 0,
                        responseTime: `${Date.now() - startTime}ms`,
                        cached: false,
                        error: true
                    }
                };
                return;
            }
        }

        const systemPrompt = `Tu es Axilum AI, un assistant intelligent avec un syst√®me rigoureux de calcul d'hallucinations.

## Capacit√©s Sp√©ciales

**G√©n√©ration d'images** : Tu peux g√©n√©rer des images via l'API Pollinations.ai
- Quand l'utilisateur demande une image, r√©ponds avec : "Je g√©n√®re l'image : [description]"
- Le syst√®me d√©tectera automatiquement cette phrase et appellera l'API de g√©n√©ration
- L'image sera affich√©e dans le chat apr√®s g√©n√©ration
- Exemples de demandes : "g√©n√®re une image de...", "cr√©e une photo de...", "dessine-moi..."

## Contraintes de Format
- **Longueur maximale : 400 mots**
- Reste concis et pr√©cis
- Si le sujet n√©cessite plus, propose de d√©tailler un aspect sp√©cifique

## Processus en 2 √âtapes

### √âTAPE 1 : Analyse Interne (ne pas afficher √† l'utilisateur)

1. **G√©n√®re mentalement ta r√©ponse** (R) √† la question de l'utilisateur
2. **D√©compose R en affirmations atomiques** (claims) - chaque fait v√©rifiable s√©par√©ment
3. **Classifie chaque claim** :
   - SUPPORTED : Affirmation v√©rifiable et correcte selon tes connaissances
   - NOT_SUPPORTED : Affirmation incertaine ou non v√©rifiable
   - CONTRADICTORY : Affirmation fausse ou contradictoire avec tes connaissances fiables
4. **Calcule les indices** :
   - H = ((0.5 * NOT_SUPPORTED) + (1.0 * CONTRADICTORY)) / total_claims
   - C = confiance moyenne dans tes sources (0.7-0.9 pour connaissances g√©n√©rales, 0.3-0.6 pour incertain)
   - U = urgence/impact si faux (0.3-0.5 pour info g√©n√©rale, 0.7-1.0 pour m√©dical/financier)
   - Rc = 1 - C
   - CHR = 0.5 * H + 0.3 * U + 0.2 * Rc
5. **Convertis en pourcentages** :
   - HI% = round(H * 100, 1)
   - CHR% = round(CHR * 100, 1)

### √âTAPE 2 : D√©cision et R√©ponse

**Si HI% ‚â• 30% OU CHR% ‚â• 30%** :
- R√©√©cris ta r√©ponse en √©tant plus prudent
- Utilise des formulations nuanc√©es ("g√©n√©ralement", "selon", "il semble")
- √âvite les affirmations non v√©rifiables
- Ajoute un avertissement

**Si HI% < 30% ET CHR% < 30%** :
- R√©ponds normalement avec confiance

## Ta Mission

R√©ponds naturellement aux questions tout en calculant rigoureusement le risque d'hallucination.

## R√®gles de Calcul Strictes

1. **D√©composition syst√©matique** : Identifie TOUS les claims atomiques dans ta r√©ponse
2. **Classification rigoureuse** :
   - Salutations, politesse = SUPPORTED (HI = 0%)
   - Faits g√©n√©raux bien connus = SUPPORTED
   - Chiffres approximatifs sans source = NOT_SUPPORTED
   - Dates/√©v√©nements incertains = NOT_SUPPORTED
   - Affirmations fausses = CONTRADICTORY
3. **Pond√©ration r√©aliste** :
   - Conversations simples : C = 0.9, U = 0.3
   - Informations techniques : C = 0.7, U = 0.5
   - Sujets complexes/sensibles : C = 0.5, U = 0.8
4. **Seuils d'alerte** :
   - HI% < 30% ET CHR% < 30% : ‚úÖ R√©ponse fiable
   - HI% ‚â• 30% OU CHR% ‚â• 30% : ‚ö†Ô∏è R√©viser la r√©ponse et avertir

## R√®gles de Pr√©sentation

- ‚ùå **Ne JAMAIS afficher** le processus de calcul interne
- ‚ùå **Ne JAMAIS montrer** les claims d√©compos√©s
- ‚úÖ **Afficher uniquement** : r√©ponse + HI% + CHR% + alerte si n√©cessaire
- ‚úÖ **Ajouter 2-3 sources acad√©miques** seulement si HI% ou CHR% ‚â• 30%

## Format de R√©ponse OBLIGATOIRE

Structure EXACTE √† suivre :

FORMAT:
[R√©ponse conversationnelle naturelle]

---
üìä HI: X.X% ‚Ä¢ CHR: Y.Y%

[Si HI >= 30% OU CHR >= 30%]
‚ö†Ô∏è Attention : [Explication br√®ve du risque]

Sources recommand√©es :
1. [Source acad√©mique/scientifique]
2. [Source de haute autorit√©]

**Niveaux de fiabilit√© automatiques** :
- HI < 15% : Tr√®s fiable
- HI 15-30% : Fiable
- HI 30-60% : Prudence requise
- HI > 60% : Haute incertitude

## Exemples de Calcul

**Exemple 1 : Salutation simple**
Question : "Hello"
Analyse interne :
- Claims : ["je r√©ponds poliment"] = 1 claim
- Classification : SUPPORTED = 1
- H = (0.5*0 + 1.0*0) / 1 = 0
- C = 0.95, U = 0.2, Rc = 0.05
- CHR = 0.5*0 + 0.3*0.2 + 0.2*0.05 = 0.07
- HI% = 0.0%, CHR% = 7.0%

R√©ponse :
"Hello! How can I help you today?"

---
üìä HI: 0.0% ‚Ä¢ CHR: 7.0%

**Exemple 2 : Question technique avec certitude**
Question : "Qu'est-ce que Node.js ?"
Analyse interne :
- Claims : ["Node.js est un runtime JavaScript", "bas√© sur V8", "permet JS c√¥t√© serveur"] = 3 claims
- Classification : SUPPORTED = 3
- H = 0 / 3 = 0
- C = 0.85, U = 0.3, Rc = 0.15
- CHR = 0.5√ó0 + 0.3√ó0.3 + 0.2√ó0.15 = 0.12
- HI% = 0.0%, CHR% = 12.0%

**Exemple 3 : R√©ponse avec incertitude**
Question : "Combien co√ªte Azure OpenAI ?"
Analyse interne :
- Claims : ["pricing varie", "bas√© sur tokens", "varie selon mod√®le", "environ X$/1K tokens"] = 4 claims
- Classification : SUPPORTED = 3, NOT_SUPPORTED = 1 (prix approximatif)
- H = (0.5*1 + 1.0*0) / 4 = 0.125
- C = 0.6, U = 0.5, Rc = 0.4
- CHR = 0.5*0.125 + 0.3*0.5 + 0.2*0.4 = 0.2925
- HI% = 12.5%, CHR% = 29.3%

R√©ponse normale (< 30%)

**Exemple 4 : Haute incertitude n√©cessitant r√©vision**
Analyse initiale donne HI% = 45% ‚Üí R√©√©cris la r√©ponse avec plus de prudence, ajoute des sources

---

## Application Syst√©matique

Pour CHAQUE r√©ponse :
1. Fais le calcul mentalement (ne pas afficher)
2. Si HI ‚â• 30% ou CHR ‚â• 30% : r√©vise ta r√©ponse + ajoute sources
3. Affiche uniquement : r√©ponse + HI% + CHR% + alerte si n√©cessaire

Sois rigoureux dans tes calculs !`;

        // √âTAPE 1 : Pr√©parer l'historique de conversation (optimis√©)
        // conversationHistory est d√©j√† d√©clar√© plus haut pour le cache
        const messages = [{ role: 'system', content: systemPrompt }];
        
        // üöÄ OPTIMISATION : Limiter l'historique aux 15 derniers messages (30 messages au total max)
        // Cela r√©duit les tokens et am√©liore le temps de r√©ponse
        const MAX_HISTORY_MESSAGES = 15; // 15 paires user/bot = 30 messages
        const recentHistory = conversationHistory.slice(-MAX_HISTORY_MESSAGES);
        
        context.log(`üìù Total history: ${conversationHistory.length} messages, using recent: ${recentHistory.length}`);
        
        // Ajouter l'historique r√©cent des messages pr√©c√©dents
        recentHistory.forEach(msg => {
            if (msg.type === 'user' && msg.content) {
                messages.push({ role: 'user', content: msg.content });
            } else if (msg.type === 'bot' && msg.content) {
                // Nettoyer la r√©ponse du bot des m√©triques pour le contexte
                const cleanContent = msg.content
                    .replace(/\n*---[\s\S]*/g, '')
                    .replace(/\n*üìä.*?HI:.*?CHR:.*?\n*/gi, '')
                    .replace(/\n*HI:\s*[0-9.]+%.*?CHR:\s*[0-9.]+%.*?\n*/gi, '')
                    .trim();
                messages.push({ role: 'assistant', content: cleanContent });
            }
        });
        
        // Ajouter le message actuel de l'utilisateur
        messages.push({ role: 'user', content: userMessage });
        
        context.log(`üìù Conversation context: ${messages.length} messages sent to API (including system prompt)`);
        
        // √âTAPE 2 : Appeler Azure OpenAI avec l'historique complet
        // Note: GPT-5.1 ne supporte pas encore logprobs, donc d√©sactiv√© temporairement
        const response = await fetch(`${endpoint}/openai/deployments/${deploymentName}/chat/completions?api-version=2024-08-01-preview`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'api-key': apiKey
            },
            body: JSON.stringify({
                messages: messages,
                max_completion_tokens: 3000
                // logprobs: true, // D√©sactiv√© - non support√© par GPT-5.1
                // top_logprobs: 5
            })
        });

        if (!response.ok) {
            const errorText = await response.text();
            context.log.error('Azure API error:', response.status, errorText);
            throw new Error(`Azure API returned ${response.status}: ${errorText}`);
        }

        const data = await response.json();
        const agentResponse = data.choices?.[0]?.message?.content || "Je n'ai pas pu g√©n√©rer une r√©ponse.";
        const logprobs = data.choices?.[0]?.logprobs;
        
        context.log('Response generated successfully');
        context.log('Response length:', agentResponse.length);
        
        // √âTAPE 2 : Extraire la confiance objective √† partir des logprobs (si disponible)
        let objectiveConfidence = 0.75; // Valeur par d√©faut √©lev√©e pour GPT-5.1
        let confidenceSource = 'default';
        
        if (logprobs && logprobs.content) {
            // Calculer la confiance moyenne bas√©e sur les probabilit√©s r√©elles du mod√®le
            const tokenConfidences = logprobs.content
                .map(token => Math.exp(token.logprob)) // Convertir log prob en probabilit√©
                .filter(prob => prob > 0); // Filtrer les valeurs invalides
            
            if (tokenConfidences.length > 0) {
                objectiveConfidence = tokenConfidences.reduce((sum, prob) => sum + prob, 0) / tokenConfidences.length;
                confidenceSource = 'logprobs';
                context.log(`üìä Confiance objective calcul√©e (logprobs) : ${(objectiveConfidence * 100).toFixed(1)}%`);
            }
        } else {
            // Estimation heuristique bas√©e sur la longueur et la complexit√©
            const wordCount = agentResponse.split(/\s+/).length;
            const hasNumbers = /\d/.test(agentResponse);
            const hasCitations = /\[.*\]|Source|selon/i.test(agentResponse);
            
            // R√©ponses courtes et directes = confiance plus √©lev√©e
            if (wordCount < 50) objectiveConfidence = 0.85;
            else if (wordCount < 150) objectiveConfidence = 0.75;
            else objectiveConfidence = 0.70;
            
            // Ajustements
            if (hasNumbers) objectiveConfidence -= 0.05; // Chiffres = plus risqu√©
            if (hasCitations) objectiveConfidence += 0.05; // Citations = plus fiable
            
            confidenceSource = 'heuristic';
            context.log(`üìä Confiance estim√©e (heuristique) : ${(objectiveConfidence * 100).toFixed(1)}%`);
        }
        
        // √âTAPE 3 : Validation multi-mod√®le (second appel ind√©pendant)
        let validationScore = 1.0; // 1.0 = validation r√©ussie, 0.0 = contradictions d√©tect√©es
        
        const validationPrompt = `Tu es un validateur critique. Analyse la r√©ponse suivante et identifie UNIQUEMENT les affirmations factuelles incorrectes ou contradictoires.

R√©ponse √† valider :
"${agentResponse}"

R√©ponds en JSON uniquement :
{
  "incorrect_claims": ["claim 1", "claim 2"],
  "validation_score": 0.0-1.0
}

Si tout est correct, retourne : {"incorrect_claims": [], "validation_score": 1.0}`;
        
        try {
            const validationResponse = await fetch(`${endpoint}/openai/deployments/${deploymentName}/chat/completions?api-version=2024-08-01-preview`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'api-key': apiKey
                },
                body: JSON.stringify({
                    messages: [
                        { role: 'system', content: 'Tu es un validateur factuel rigoureux. R√©ponds uniquement en JSON.' },
                        { role: 'user', content: validationPrompt }
                    ],
                    max_completion_tokens: 500,
                    temperature: 0.2 // Temp√©rature basse pour validation stricte
                })
            });
            
            if (validationResponse.ok) {
                const validationData = await validationResponse.json();
                const validationContent = validationData.choices?.[0]?.message?.content || '{}';
                
                // Parser le JSON de validation
                try {
                    const jsonMatch = validationContent.match(/\{[\s\S]*\}/);
                    if (jsonMatch) {
                        const validation = JSON.parse(jsonMatch[0]);
                        validationScore = validation.validation_score || 1.0;
                        
                        if (validation.incorrect_claims && validation.incorrect_claims.length > 0) {
                            context.log(`‚ö†Ô∏è Validation d√©tect√©e : ${validation.incorrect_claims.length} affirmations douteuses`);
                            context.log('Affirmations douteuses:', validation.incorrect_claims);
                        } else {
                            context.log('‚úÖ Validation r√©ussie : aucune contradiction d√©tect√©e');
                        }
                    }
                } catch (parseError) {
                    context.log.warn('Impossible de parser la validation JSON:', parseError.message);
                }
            }
        } catch (validationError) {
            context.log.warn('√âchec de la validation multi-mod√®le:', validationError.message);
        }
        
        // √âTAPE 3.5 : V√©rification RAG et Fact-Checking externe
        let ragResults = { enriched: false };
        let factCheckResults = { checked: false };
        
        try {
            // RAG : Enrichir avec base de connaissances interne
            if (ragSystem.enabled) {
                ragResults = await ragSystem.enrichContext(userMessage, agentResponse);
                
                if (ragResults.enriched) {
                    context.log(`üìö RAG: ${ragResults.relevantFacts.length} faits pertinents trouv√©s`);
                    
                    if (ragResults.hasContradictions) {
                        context.log(`‚ö†Ô∏è RAG: ${ragResults.contradictions.length} contradictions d√©tect√©es !`);
                        // R√©duire score de validation si contradictions KB
                        validationScore = Math.min(validationScore, 0.6);
                    }
                }
            }
            
            // Fact-Checking : V√©rifier contre sources publiques (Google Fact Check)
            if (factChecker.enabled) {
                factCheckResults = await factChecker.checkText(agentResponse);
                
                if (factCheckResults.checked) {
                    context.log(`üîç Fact-Check: ${factCheckResults.claimsFound} claims extraits`);
                    
                    if (factCheckResults.hasFakeNews) {
                        context.log(`üö® FAKE NEWS D√âTECT√âE dans la r√©ponse !`);
                        // R√©duction drastique de confiance si fake news
                        objectiveConfidence = Math.min(objectiveConfidence, 0.3);
                        validationScore = Math.min(validationScore, 0.3);
                    } else if (factCheckResults.claimsVerified > 0) {
                        context.log(`‚úÖ Fact-Check: ${factCheckResults.claimsVerified} claims v√©rifi√©s (trust: ${(factCheckResults.overallTrust * 100).toFixed(0)}%)`);
                        // Bonus de confiance si claims v√©rifi√©s positivement
                        objectiveConfidence = Math.min(1.0, objectiveConfidence + 0.05);
                    }
                }
            }
        } catch (ragError) {
            context.log.warn('Erreur lors du RAG/Fact-Checking:', ragError.message);
        }
        
        // √âTAPE 4 : Ajouter √† l'historique pour apprentissage adaptatif
        responseHistory.add({
            confidence: objectiveConfidence,
            validation: validationScore,
            messageLength: userMessage.length
        });
        
        const historyStats = responseHistory.getStats();
        const adaptiveThreshold = responseHistory.getAdaptiveThreshold();
        
        context.log(`üìà Statistiques historiques: ${historyStats.sampleSize} entr√©es, conf moy: ${historyStats.avgConfidence}, val moy: ${historyStats.avgValidation}`);
        context.log(`üéØ Seuil adaptatif actuel: ${(adaptiveThreshold * 100).toFixed(0)}%`);
        
        // √âTAPE 5 : Enrichir la r√©ponse avec les m√©triques objectives
        const enrichedResponse = {
            response: agentResponse,
            model: deploymentName,
            source: 'axilum-ai-gpt5-enhanced',
            timestamp: new Date().toISOString(),
            confidence_metrics: {
                objective_confidence: Math.round(objectiveConfidence * 100) / 100,
                confidence_source: confidenceSource,
                validation_score: Math.round(validationScore * 100) / 100,
                confidence_level: objectiveConfidence >= 0.8 ? 'high' : objectiveConfidence >= 0.6 ? 'medium' : 'low',
                validation_status: validationScore >= 0.9 ? 'validated' : validationScore >= 0.7 ? 'minor_concerns' : 'major_concerns',
                adaptive_threshold: adaptiveThreshold,
                historical_stats: historyStats
            }
        };
        
        // Ajouter les r√©sultats RAG et Fact-Checking si disponibles
        if (ragResults.enriched) {
            enrichedResponse.rag_verification = {
                enabled: true,
                relevant_facts_count: ragResults.relevantFacts.length,
                contradictions_found: ragResults.contradictions.length,
                recommendation: ragResults.recommendation,
                top_facts: ragResults.relevantFacts.slice(0, 2).map(f => ({
                    fact: f.fact,
                    confidence: f.confidence,
                    similarity: Math.round(f.similarity * 100) / 100
                }))
            };
        }
        
        if (factCheckResults.checked && factCheckResults.claimsVerified > 0) {
            enrichedResponse.fact_check = {
                enabled: true,
                claims_extracted: factCheckResults.claimsFound,
                claims_verified: factCheckResults.claimsVerified,
                overall_trust: Math.round(factCheckResults.overallTrust * 100) / 100,
                fake_news_detected: factCheckResults.hasFakeNews,
                verified_claims: factCheckResults.results
                    .filter(r => r.found)
                    .map(r => ({
                        claim: r.claim.substring(0, 100) + (r.claim.length > 100 ? '...' : ''),
                        rating: r.rating,
                        publisher: r.publisher,
                        trust_score: r.trustScore
                    }))
            };
        }
        
        context.log('üìä M√©triques finales:', enrichedResponse.confidence_metrics);
        
        // üõ°Ô∏è Analyse de protection contre l'accumulation d'hallucinations
        try {
            // conversationHistory est d√©j√† d√©clar√© plus haut
            const currentHI = hiPercentage;
            const messagesWithCurrent = [
                ...conversationHistory,
                {
                    type: 'bot',
                    hiScore: currentHI,
                    chrScore: chrPercentage,
                    content: enrichedResponse.response
                }
            ];
            
            const protectionAnalysis = hallucinationProtection.analyzeConversationRisk(
                req.body.conversationId || 'default',
                messagesWithCurrent
            );
            
            // Ajouter l'analyse de protection √† la r√©ponse
            enrichedResponse.protection = {
                risk_level: protectionAnalysis.level,
                should_intervene: protectionAnalysis.shouldIntervene,
                should_block: protectionAnalysis.shouldBlock,
                stats: protectionAnalysis.stats,
                recommended_action: protectionAnalysis.action
            };
            
            context.log('üõ°Ô∏è Protection analysis:', protectionAnalysis.level);
        } catch (protectionError) {
            context.log.error('‚ö†Ô∏è Erreur dans l\'analyse de protection:', protectionError.message);
            // Protection par d√©faut en cas d'erreur
            enrichedResponse.protection = {
                risk_level: 'safe',
                should_intervene: false,
                should_block: false,
                stats: { avgHI: 0, maxHI: 0, recentAvgHI: 0, totalMessages: 0, highRiskCount: 0, trend: 'stable' },
                recommended_action: { type: 'NONE', message: '', description: '', actions: [], icon: '', color: '' }
            };
        }
        
        // üöÄ PERFORMANCE : Calculer le temps de traitement total
        const totalTime = Date.now() - startTime;
        enrichedResponse.processingTime = `${(totalTime / 1000).toFixed(2)}s`;
        enrichedResponse.processingTimeMs = totalTime;
        
        // üíæ CACHE : Stocker la r√©ponse en cache (seulement pour conversations sans historique ou courtes)
        if (conversationHistory.length <= 5) {
            setCachedResponse(cacheKey, enrichedResponse);
            context.log(`üíæ Response cached for key: ${cacheKey.substring(0, 50)}...`);
        }
        
        context.log(`‚ö° Total processing time: ${totalTime}ms`);
        context.log(`üìä Cache size: ${responseCache.size} entries`);
        
        context.res = {
            status: 200,
            headers: {
                'Content-Type': 'application/json',
                'X-Cache': 'MISS',
                'X-Response-Time': `${totalTime}ms`
            },
            body: enrichedResponse
        };

    } catch (error) {
        const errorTime = Date.now() - startTime;
        context.log.error('Error invoking Axilum AI:', error);
        context.log.error('Error stack:', error.stack);
        context.log.error(`‚ùå Error occurred after ${errorTime}ms`);
        context.res = {
            status: 500,
            headers: { 
                'Content-Type': 'application/json',
                'X-Response-Time': `${errorTime}ms`
            },
            body: { 
                error: "Failed to invoke Axilum AI",
                details: error.message,
                timestamp: new Date().toISOString(),
                processingTimeMs: errorTime
            }
        };
    }
};
